<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>从1G到6G分别采用了哪种信道共享技术？</title>
    <url>/2020/12/04/1G-6G/</url>
    <content><![CDATA[<p>　　在计算机网络中使用的信道共享技术可以分为三种，即随机接入、受控接入和信道复用。</p>
<a id="more"></a>

<ul>
<li><p>随机接入：所有的用户都可以根据自己的意愿随机地向信道上发送信息。当两个或两个以上的用户都在共享的信道上发送信息的时候，就产生了冲突，它导致用户的发送失败。随机接入技术主要就是研究解决冲突的网络协议。</p>
</li>
<li><p>受控接入：各个用户不能随意接入信道而必须服从一定的控制。分为集中式控制和分散式控制。集中式控制的主要方法是轮询技术，分散式控制的主要方法有令牌技术。</p>
</li>
<li><p>信道复用：信道复用主要用于将多个低速信号组合为一个混合的高速信号后，在高速信道上传输。分为频分复用，时分复用，波分复用，码分复用，空分复用，统计复用，极化波复用。</p>
</li>
</ul>
<p>　　从1G到6G,手机通信系统需要解决的问题有：远距离传输、传输速度快、实现大量传输、保证传输的正确性、保证传输的安全性。</p>
<ul>
<li>为了使得传输距离更远，主要的问题就是解决干扰。干扰的存在导致距离不能传远，但是也不能不存在干扰，若无干扰则信号永远不会消失。原始的信号通常频率很低，在信道中传输损耗较大，因此不宜直接进行传输，所以需要进行信号调制。通过调制，将信号的通频带落到传输介质的通频带中，转换成适用于信道传输的信号。</li>
<li>为了实现大量传输，则需要使用信道复用技术。有时分复用，空分复用，频分复用和码分复用。</li>
</ul>
<p>　　1G：1G时代仅限语音通话，此时是模拟通信，采用频分复用技术，将频率分割成多个片段。存在保密性差，信号不稳定，容易受噪声干扰且无法还原的问题。</p>
<p>　　2G：2G可使用语音和文字，此时采用数字通信。若信号的频率为f，只要发送≥2f个点就可以将信号还原，抗干扰能力增强。采用频分复用和时分复用技术。</p>
<p>　　3G：3G加入图像，音乐等媒体，采用码分复用技术。</p>
<p>　　4G：采用码分复用，空分复用技术和长期演进技术。</p>
<p>　　5G：采用频分复用、时分复用、空分复用、码分复用和Massive MIMO。</p>
<p>　　6G：采用太赫兹技术。</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>1G,2G,3G,4G,5G,6G</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda中为每个项目创建新环境</title>
    <url>/2020/12/04/Anaconda%E4%B8%AD%E4%B8%BA%E6%AF%8F%E4%B8%AA%E9%A1%B9%E7%9B%AE%E5%88%9B%E5%BB%BA%E6%96%B0%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="创建新环境"><a href="#创建新环境" class="headerlink" title="创建新环境"></a>创建新环境</h1><p>进入cmd，进入anaconda所在的文件夹位置（如：D:\Software\Anaconda\envs）</p>
<a id="more"></a>

<p>依次输入：</p>
<p><code>conda create --name 项目名称 python=版本号</code></p>
<p><code>activate</code></p>
<p><code>conda activate 项目名称</code>   切换不同环境的时候也使用该语句</p>
<p>然后使用pip install安装所需包即可。</p>
<hr>
<h1 id="查看环境"><a href="#查看环境" class="headerlink" title="查看环境"></a>查看环境</h1><ul>
<li>查看目前已安装的环境：<code>conda info --envs</code></li>
<li>查看环境下已安装的包：<code>pip list</code></li>
</ul>
]]></content>
      <categories>
        <category>开发环境</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读——BBN Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</title>
    <url>/2021/03/22/BBN%20Bilateral-Branch%20Network%20with%20Cumulative%20Learning%20for%20Long-Tailed%20Visual%20Recognition/</url>
    <content><![CDATA[<h3 id="BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition"><a href="#BBN-Bilateral-Branch-Network-with-Cumulative-Learning-for-Long-Tailed-Visual-Recognition" class="headerlink" title="BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition"></a>BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</h3><h5 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h5><p>​    处理长尾分布问题通常的措施是采用重平衡策略（重加权和重采样），这些重平衡方法能够有效促进深层网络的分类器学习，但同时会在一定程度上损害深度网络学习到的特征。<a id="more"></a>因此本文提出了一个双分支网络（BBN）同时处理表征学习和分类器学习。该模型具有累积学习策略，首先学习通用模式，然后逐步关注尾部数据。</p>
<h5 id="Re-balancing-strategies"><a href="#Re-balancing-strategies" class="headerlink" title="Re-balancing strategies"></a>Re-balancing strategies</h5><p>​    将深度分类模型分为两个部分：特征提取和分类器。本文设计了一个两阶段的实验方式，分别学习深层模型的表征和分类器。第一阶段采用传统的CE（交叉熵损失）、RW(重加权)、RS（重采样）三种训练策略训练模型。第二阶段固定特征提取部分，按照第一阶段的训练策略训练分类器。结果如下图。</p>
<p><img src="http://jiajunjun.top/md_picture/BBN/1.png" alt="image-20210322111031941"></p>
<p>​    结果表明：固定表示学习，在垂直方向上使用重平衡策略有效。但在水平方向上，固定分类器后，使用重平衡策略的结果更差。表明学习深层特征的辨别能力更差。</p>
<h5 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h5><p><img src="http://jiajunjun.top/md_picture/BBN/2.png" alt="image-20210322111314609"></p>
<p>​    在数据采样阶段，传统学习分支使用正常的uniform sampler，而re-blancing分支使用reversed sampler。反向采样器中每个类的采样可能性与其样本大小的倒数成正比。采样可能性的计算如下式：</p>
<p><img src="http://jiajunjun.top/md_picture/BBN/3.png" alt="image-20210322112154347"></p>
<p>​    使用ResNet作为主干网络，除了最后一个剩余块，两个分支网络共享相同的权重。共享权重有两个好处:一方面，传统学习分支的良好学习表示可以有利于重新平衡分支的学习。另一方面，共享权重将大大降低推理阶段的计算复杂度。</p>
<p>​    积累学习策略：通过控制两个分支产生的特征的权重和分类损失来转移双边分支之间的学习焦点。首先学习通用模式，然后逐渐关注尾部数据。在训练阶段，传统学习分支的特征乘以α，重平衡分支的特征乘以1-α，α根据训练时间自动生成。α随着训练时间的增加而逐渐减小。</p>
<p><img src="http://jiajunjun.top/md_picture/BBN/4.png" alt="image-20210322144827580"></p>
<h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>数据集：Long-tailed CIFAR-10 和 CIFAR-100</p>
<p>​                iNaturalist 2017 and iNaturalist 2018.</p>
<p><img src="http://jiajunjun.top/md_picture/BBN/5.png" alt="image-20210322145244397"></p>
<p><img src="http://jiajunjun.top/md_picture/BBN/6.png" alt="image-20210322145305351"></p>
<p><img src="http://jiajunjun.top/md_picture/BBN/7.png" alt="image-20210322145410182"></p>
<p><img src="http://jiajunjun.top/md_picture/BBN/8.png" alt="image-20210322145439154"></p>
]]></content>
      <categories>
        <category>长尾分布</category>
      </categories>
      <tags>
        <tag>Few-Shot Learning</tag>
        <tag>Long-Tailed</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读：Decoupling Representation and Classifier for Long-Tailed Recognition</title>
    <url>/2021/03/23/Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition/</url>
    <content><![CDATA[<h2 id="Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition"><a href="#Decoupling-Representation-and-Classifier-for-Long-Tailed-Recognition" class="headerlink" title="Decoupling Representation and Classifier for Long-Tailed Recognition"></a>Decoupling Representation and Classifier for Long-Tailed Recognition</h2><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>​    长尾分布的问题通常是由类平衡策略解决的。本文将学习过程分解为表征学习和分类两个部分，并系统地探讨了不同的平衡策略如何影响它们的长尾识别。</p>
<a id="more"></a>

<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>​    首先训练模型学习在不同采样策略下（基于实例的采样、类平衡采样、上述两种的混合）的表征，然后研究了三种不同的方法基于学习表征来获得具有平衡边界的分类器，分别是1)以类平衡的方式重新训练参数线性分类器(即，重新采样)；2)非参数最近类均值分类器，其基于来自训练集的最接近的类特定均值表示对数据进行分类；以及3)归一化分类器权重，这直接调整权重大小以更加平衡，添加温度以调节归一化过程。</p>
<h4 id="Learning-representations-for-long-tailed-recognition"><a href="#Learning-representations-for-long-tailed-recognition" class="headerlink" title="Learning representations for long-tailed recognition"></a>Learning representations for long-tailed recognition</h4><p>​    把表征学习从分类器中分离出来。</p>
<p>​    <u>采样策略</u>：从第j类数据点采样的概率为<img src="http://jiajunjun.top/md_picture/Decoupling/1.png" alt="image-20210323104809053"></p>
<p>​    <strong>样本均衡采样</strong>：q=1,每个训练样本都有相同的概率被采样。</p>
<p>​    <strong>类别均衡采样</strong>：q=0,每个类别有相同的概率被采样。首先先选择类，然后在对类里的实例进行采样。</p>
<p>​    <strong>平方根采样</strong>：q=0.5。</p>
<p>​    <strong>渐进均衡采样</strong>：根据训练epoch，结合样本均衡采样和类别均衡采样，赋予不同的权重决定采样的概率。</p>
<p><img src="http://jiajunjun.top/md_picture/Decoupling/2.png" alt="image-20210323110122564"></p>
<p>​    <u>损失重加权策略</u>：对不平衡数据的损失重新加权。</p>
<h4 id="Classification-for-long-tailed-recognition"><a href="#Classification-for-long-tailed-recognition" class="headerlink" title="Classification for long-tailed recognition"></a>Classification for long-tailed recognition</h4><p>​    通过使用不同的采样策略或其他非参数方法(如最近类均值分类器)进行微调来校正头尾类的决策边界</p>
<p>​    <strong>重训练分类器</strong>：用类别均衡采样重新训练分类器。保持表征固定，随机重新初始化优化分类器W和b。</p>
<p>​    <strong>最近类均值分类器</strong>：首先计算训练集上每个类的平均特征表示，然后使用余弦相似性或在L2归一化平均特征上计算的欧几里德距离来执行最近邻搜索判断。</p>
<p>​    <strong>τ-归一化分类器</strong>：通过以下τ-归一化过程直接调整分类器权重范数来纠正决策边界的不平衡。当τ = 1时，它简化为标准L2归一化，当τ = 0时，不施加缩放。丢弃偏置项b。</p>
<p><img src="http://jiajunjun.top/md_picture/Decoupling/3.png" alt="image-20210323111247999"></p>
<p>​    <strong>可学习的权重比例</strong>：解释τ-归一化的另一种方法是将其视为每个分类器大小的重新缩放，同时保持方向不变。</p>
<p><img src="http://jiajunjun.top/md_picture/Decoupling/4.png" alt="image-20210323111539498"></p>
<h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p>数据集：Places-LT，ImageNet-LT ，iNaturalist 2018 </p>
<p><img src="http://jiajunjun.top/md_picture/Decoupling/5.png" alt="image-20210323111836493"></p>
]]></content>
      <categories>
        <category>长尾分布</category>
      </categories>
      <tags>
        <tag>Few-Shot Learning</tag>
        <tag>Long-Tailed</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读：Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</title>
    <url>/2021/03/21/Few-Shot%20and%20Zero-Shot%20Multi-Label%20Learning%20for%20Structured%20Label%20Spaces/</url>
    <content><![CDATA[<h3 id="Few-Shot-and-Zero-Shot-Multi-Label-Learning-for-Structured-Label-Spaces"><a href="#Few-Shot-and-Zero-Shot-Multi-Label-Learning-for-Structured-Label-Spaces" class="headerlink" title="Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces"></a>Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces</h3><h5 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h5><p>​    对于文本分类问题，多标签的少样本或者无样本的标签预测很少被探索。在本文中，执行了一个细粒度的评估来理解最新的方法如何在不常见的标签上执行。此外，当标签空间上存在已知结构时，开发了多标签文本分类的少样本和无样本方法。</p>
<a id="more"></a>

<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>​    大规模多标签文本分类存在的两个问题：</p>
<p>​    1.在大型文档中查找特定标签的相关信息很难；</p>
<p>​    2.数据稀疏的问题，随着标签总数的增加，一些标签可能会频繁出现，但是大多数标签不会频繁出现。</p>
<p>​    few-shot learning是指在训练数据集中每个标签只有几个例子（通常在1~5之间）可用的监督方法。</p>
<p>​    zero-shot learning是指在测试时想要预测的标签在训练数据中没有可用的。</p>
<p>​    本文提出并评估了一种神经网络结构，用于预测多标签集合中的少样本和零样本标签；针对频繁、少样本和零样本标签，通过使用扩展的广义零样本方法评估幂律数据集。</p>
<h5 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h5><p><img src="http://jiajunjun.top/md_picture/FSL1/1.png" alt="图1"></p>
<p>1.生成标签向量</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/2.png" alt="图2"></p>
<p>2.为每一个标签生成特定的文本向量表示</p>
<p>​    使用标签的注意力避免长文档中遇到的大海捞针的情况。首先通过一个前馈神经网络传递文本特征矩阵D:</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/3.png" alt="图3"></p>
<p>​    生成label-wise注意力向量ai∈Rn-s+1，表示对于某个标签，每一个ngram的信息量。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/4.png" alt="图4"></p>
<p>​    为每一个标签生成特定的文本向量表示ci∈Ru，D中所有行的加权平均。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/5.png" alt="图5"></p>
<p>3..标签向量通过两层GCNN传递，包含关于标签空间的分层信息。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/6.png" alt="图6"></p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/7.png" alt="图7"></p>
<p>​    然后将平均描述向量与GCNN标签向量连接起来</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/8.png" alt="图8"></p>
<p>​    为了比较最终的标签向量v3和它的文档向量ci，我们将文档向量转换成</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/9.png" alt="图9"></p>
<p>​    预测</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/10.png" alt="图10"></p>
<p>4.训练</p>
<p>使用多标签二元交叉熵损失来训练模型</p>
<p><img src="http://jiajunjun.top/md_picture/FSL1/11.png" alt="图11"></p>
<p>参考文章：<a href="https://blog.csdn.net/Daffy10/article/details/109588275">https://blog.csdn.net/Daffy10/article/details/109588275</a></p>
]]></content>
      <categories>
        <category>小样本学习</category>
      </categories>
      <tags>
        <tag>Few-Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Few-Shot Learning</title>
    <url>/2021/03/22/Few-shot%20Learning/</url>
    <content><![CDATA[<h1 id="Few-shot-Learning"><a href="#Few-shot-Learning" class="headerlink" title="Few-shot Learning"></a>Few-shot Learning</h1><p>​    在我们所熟知的深度学习领域，可以通过大量的训练集训练模型进行分类和回归。但当有标注的数据很少的时候该如何解决？Few-shot learning则是解决该问题的一个领域，就是用很少的样本进行分类或回归。</p>
<a id="more"></a>

<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>​    首先观察下边这幅图，左边是犰狳，右边是穿山甲。    </p>
<p><img src="http://jiajunjun.top/md_picture/FSL/1.png" alt="image-20210322194447292"></p>
<p>​    现在给出一幅Query图片，判断这是犰狳还是穿山甲。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/2.png" alt="image-20210322194636385"></p>
<p>​    我们可以很容易判断出上述图片是穿山甲。人类可以很容易的从少量样本中学习到信息，通过比对得出图片所属类别。Few-shot learning不是让机器识别训练集中的图片且泛化到测试集，而是学会如何学习。比如我们人类的思维是通过比较Query图片与上述Support set中的哪一类图片更相似从而得出正确的结论。所以类比到机器，只需要让机器learn to learn，然后判断下边的图片是不是同一类。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/3.png" alt="image-20210322195524598"></p>
<p>​    当直接问神经网络下图中的Query是什么动物时，神经网络不能判别，但当给出下图的Support Set后再次让神经网络判断，得出Query是Otter.</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/4.png" alt="image-20210322195735615"></p>
<p>​    让机器learn to learn 的过程就是meta learning。meta learning希望机器有自主学习的能力。就是即使没见过Query图片上的动物，但通过Support set上的动物便可以对所要分类的动物进行分辨的能力。</p>
<p>​    few-shot learning中有两个常用的术语：k-way和n-shot。k-way指的是support set中有k个类，n-shot指的是每个类中含有n个样本。上图就是一个6 way 1 shot的例子。</p>
<p>​    整个few-learning的过程如下，首先在一个大规模的数据集上学习辨别相似度的能力。然后使用学习到的相似度函数对图片进行预测。通过对Query和Support set中的图片一一对比，分别计算出相似度，从而进行判断，把相似度最高的作为图片的分类结果。</p>
<h4 id="孪生网络（siamese-network"><a href="#孪生网络（siamese-network" class="headerlink" title="孪生网络（siamese network)"></a>孪生网络（siamese network)</h4><p>​    首先构建数据集，需要构造正样本和负样本。正样本使得神经网络辨别哪些东西属于同一类，负样本使得神经网络具有辨别差异的能力。正样本的选取是随机从数据集上选取一张图片，然后从该图片所属的类中随机抽取另一张，构成正样本，标签为1。负样本的选取是随机从数据集上选取一张图片，然后从除该图片所属类别外的其他类别随机抽取一张图片，构成负样本，标签为0。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/5.png" alt="image-20210322201253073"></p>
<p>​    然后通过CNN提取图片的特征。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/6.png" alt="image-20210322201346084"></p>
<p>​    现在开始训练神经网络，例如选出刚刚数据集中标签为1的数据，分别将两张图片放入同一个CNN中提取特征，之后判断相似度，图片中的Z=|h1- h2|,再用全连接层进行处理得到一个标量，最后使用sigmoid函数输出一个0到1之间的概率值，与其标签1进行比对，做损失函数预测标签和预测值的差别，然后反向更新全连接层和CNN的参数。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/7.png" alt="image-20210322201514340"></p>
<p>​    类似的，训练标签为0的数据。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/8.png" alt="image-20210322201915752"></p>
<p>​    训练完成后便可进行预测。给定一个Support set，support set中的所有类别都没有在训练集中出现过，现在确定Query中的图片属于Support set中的某一类，通过分别与Support set中的每一类计算相似度，最终得出Query与support set中的squirrel相似度最高，因此把它归为squirrel。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/9.png" alt="image-20210322202015460"></p>
<p>​    除上述训练孪生网络的方法外，还有另一种方法，Triplet Loss。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/10.png" alt="image-20210322202439744"></p>
<p>​    首先从数据集中任意选取一张图片做为anchor，从同一类中抽取一张图片作为正样本，从其他类中抽取一张图片作为负样本，然后通过同一个卷积神经网络提取特征，分别计算正样本与锚点之间的距离以及负样本与锚点之间的距离。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/11.png" alt="image-20210322202725837"></p>
<p>​    我们的目标是使得d+尽可能小，d-尽可能大，因此如下图所示定义损失函数。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/12.png" alt="image-20210322202908782"></p>
<p>​    然后比较Query与support set的每一类的距离得出结论。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/13.png" alt="image-20210322203025495"></p>
<h4 id="Pretraining-和-fine-tuning"><a href="#Pretraining-和-fine-tuning" class="headerlink" title="Pretraining 和 fine tuning"></a>Pretraining 和 fine tuning</h4><h5 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h5><p>​    假设有两个单位向量，它们的夹角记为θ，此时余弦相似度为： <img src="http://jiajunjun.top/md_picture/FSL/14.png" alt="[公式]"> ，其实就是x在w方向上的投影长度，因此它的取值范围就是[-1,1]。如果两个向量不是单位向量，此时就需要对其进行归一化。此时计算余弦相似度的公式为： <img src="http://jiajunjun.top/md_picture/FSL/15.png" alt="[公式]"></p>
<p>​    对于一个3-way,2-shot的例子，首先对于每个类别中的两个样本提取特征，然后将每个类别的两个特征求平均值和归一化，得到三个类别的表征：μ1，μ2，μ3。做分类的时候将Query的特征向量q与三个表征计算余弦相似度。使用P=Softmax(Mq)计算概率，其中M是三个表征向量μ1，μ2，μ3组成的矩阵。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/16.png" alt="image-20210322203735265"></p>
<p>​    通常在第一步预训练和第二步进行预测的中间加一步，即为在小样本数据集上进行fine-tuning，我们之前预测的时候softmax函数中的权重W采用M，偏置b设为0，现在我们把support set中的所有数据输入训练参数W和b，通常加一个正则化防止过拟合。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/17.png" alt="image-20210322204534008"></p>
<p>​    fine - tuning有三个技巧，一是初始化权重为M，二是正则化采用Entropy regularization，三是使用余弦距离和softmax分类器提高准确率。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL/18.png" alt="image-20210322204735204"></p>
<p><img src="http://jiajunjun.top/md_picture/FSL/19.png" alt="image-20210322205108258"></p>
]]></content>
      <categories>
        <category>小样本学习</category>
      </categories>
      <tags>
        <tag>Few-Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读——Learning to Propagate Labels： Transductive Propagation Network for Few-shot Learning</title>
    <url>/2021/03/23/Learning-to-propagate-labels/</url>
    <content><![CDATA[<h3 id="Learning-to-Propagate-Labels-Transductive-Propagation-Network-for-Few-shot-Learning"><a href="#Learning-to-Propagate-Labels-Transductive-Propagation-Network-for-Few-shot-Learning" class="headerlink" title="Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning"></a>Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning</h3><h5 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h5><p>​    few-shot learning 通常引用元学习的方法解决，但仍然存在数据少的问题。本文提出了一种新的元学习框架——TPN，TPN通过学习采用数据中的流形结构的图构造模块，学习将标签从已标记的实例传播到未标记的实例。</p>
<a id="more"></a>

<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>​    本文采用直推式推导解决低数据量的问题</p>
<p><img src="http://jiajunjun.top/md_picture/FSL2/1.png" alt="image-20210322095424581"></p>
<p>​    首先使用深度神经网络将输入映射到嵌入空间。然后提出了一个图构造模块，利用支持集和查询集的结合来开发新类空间的流形结构。根据图的结构，应用迭代标签传播将标签从支持集传播到查询集，最终得到一个封闭形式的解。利用查询集的传播得分和基本真值标签，我们计算了与特征嵌入和图构造参数相关的交叉熵损失。最后，所有参数都可以使用反向传播进行端到端更新。</p>
<h5 id="Main-Approach"><a href="#Main-Approach" class="headerlink" title="Main Approach"></a>Main Approach</h5><p><img src="http://jiajunjun.top/md_picture/FSL2/2.png" alt="image-20210322095651073"></p>
<h6 id="Feature-Embedding"><a href="#Feature-Embedding" class="headerlink" title="Feature Embedding"></a>Feature Embedding</h6><p>​    通过CNN对输入样本进行特征提取生成Embedding编码</p>
<h6 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h6><p><img src="http://jiajunjun.top/md_picture/FSL2/3.png" alt="image-20210322100144958"></p>
<p>​    构建无向图，根据上述公式计算权重。</p>
<h6 id="Label-Propagation"><a href="#Label-Propagation" class="headerlink" title="Label Propagation"></a>Label Propagation</h6><p>​    <img src="http://jiajunjun.top/md_picture/FSL2/4.png" alt="image-20210322100334591"></p>
<p>​    S表示归一化权重，α∈（0，1）控制传播的信息量，通过上式进行标签传播。</p>
<h6 id="Classfication-Loss-Generation"><a href="#Classfication-Loss-Generation" class="headerlink" title="Classfication Loss Generation"></a>Classfication Loss Generation</h6><p>​    计算交叉熵损失，反向传播更新参数。</p>
<h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>数据集：</p>
<p>​    采用miniImageNet 和 tieredImageNet</p>
]]></content>
      <categories>
        <category>小样本学习</category>
      </categories>
      <tags>
        <tag>Few-Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读：Optimization as a model for few-shot learning</title>
    <url>/2021/03/21/Optimization%20as%20a%20model%20for%20few-shot%20learning/</url>
    <content><![CDATA[<h5 id="Optimization-as-a-model-for-few-shot-learning"><a href="#Optimization-as-a-model-for-few-shot-learning" class="headerlink" title="Optimization as a model for few-shot learning"></a>Optimization as a model for few-shot learning</h5><h5 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h5><p>​    提出基于LSTM的元学习者模型来学习精确的优化算法，用于训练另一个在少样本的情况下的学习者神经网络分类器。该模型能够学习适当的参数更新，还可以学习学习者网络的一般初始化，以便快速收敛训练。</p>
<a id="more"></a>

<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>​    优化深度的、高容量的模型需要在许多标记的例子中进行多次迭代更新。但目前拥有的是一组数据集，每个数据集都有很少的带注释的实例。提出了一种基于LSTM的元学习者优化器，他经过训练来优化学习者神经网络分类器。元学习者既掌握任务中的短期知识，也掌握所有任务中普遍存在的长期知识。元学习者模型被训练为在每个任务上快速收敛学习者分类器到一个良好的解。此外，元学习者模型的形成允许它学习一个任务通用的初始化，它捕获所有任务之间共享的基本知识。</p>
<p>​    基于梯度下降的优化算法在少样本问题上会导致失效，是由于基于梯度的下降算法都不是专门设计以用于少样本少量参数更新迭代的，还有就是对于分离的多个任务，网络参数通常会从一个随机的初始化参数开始训练，影响在少量参数更新迭代之后的表现。</p>
<h5 id="Task-Description"><a href="#Task-Description" class="headerlink" title="Task Description"></a>Task Description</h5><p>​    机器学习中通常对一个数据集D进行划分，然后在训练集上优化参数，在测试集上评估泛化能力。但是在元学习中，处理的是包含多个正则数据集的元集D<del>,每个D∈D</del>都包含训练集和测试集。</p>
<p>​    对于一个K-shot,N-class的分类任务，训练集由N个类组成，每个类包含K个带标签的例子，即训练集包含K*N个数据。</p>
<p>​    在元学习中，有不同的元集用于元训练、元验证和元测试。在元训练中训练一个元学习者，它可以把其中一个训练集作为输入，生成一个在相应的测试集上获得高平均分类性能的分类器（学习者）。使用元验证集对元学习者进行超参数选择，并在元测试集上评价其泛化性能。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/1.png" alt="图1"></p>
<h5 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h5><h6 id="Model-Description"><a href="#Model-Description" class="headerlink" title="Model Description"></a>Model Description</h6><p>常规的梯度下降优化方法是：</p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/2.png" alt="image-20210321192958948"></p>
<p>​    这与LSTM中的单元状态的更新很类似，</p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/3.png" alt="image-20210321193052220"></p>
<p>​    因此提出一个使用LSTM的元学习者来学习一个更新规则。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/4.png" alt="image-20210321193604250"></p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/5.png" alt="image-20210321193611574"></p>
<p>​    通过上述式子决定学习率it可以控制任务学习者在少样本学习任务上快速学习同时避免发散。对于ft，需要收缩当前的参数并忘掉部分之前的参数来逃离当前的局部最优。</p>
<p>​    这样不仅学习了一个新的优化过程，还学习了一个作为任务学习者的初始化参数，这个初始化参数可以作为一个最佳的训练起点，加速优化过程。</p>
<h6 id="Parameter-sharing-amp-Preprocessing"><a href="#Parameter-sharing-amp-Preprocessing" class="headerlink" title="Parameter sharing &amp; Preprocessing"></a>Parameter sharing &amp; Preprocessing</h6><p>​    为了避免元学习者的参数爆炸，采用了在任务学习者的梯度坐标上共享参数。将输入作为一批梯度坐标和每个维度的损失输入。</p>
<p>​    由于不同坐标的梯度和损失大小可能不同，所以进行了一个标准化。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/6.png" alt="image-20210321194635027"></p>
<h6 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h6><p>​    在多个元训练集上进行训练，然后学习到所需要的更新规则和初始化，面对一个新的少样本任务是，在这个任务的support set上使用更新规则对这个初始化参数进行优化更新，在query set上进行性能评估。</p>
<h5 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h5><p>数据集：MIMIC Ⅱ 和MIMIC Ⅲ（包含标注诊断和手术标签的出院总结），根据训练数据集中的频分为三组：频繁组（大于5），少样本组（1~5），零样本组。</p>
<p>评估方法：同一个实例可以出现在所有的组中，多标签；采用R@k处理多个标签。</p>
<p><img src="http://jiajunjun.top/md_picture/FSL3/7.png" alt="image-20210322093249973"></p>
]]></content>
      <categories>
        <category>小样本学习</category>
      </categories>
      <tags>
        <tag>Few-Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>从网络体系架构的角度分析，P2P技术的本质是什么？</title>
    <url>/2020/12/16/P2P%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9C%AC%E8%B4%A8/</url>
    <content><![CDATA[<p>　　P2P可以理解为对等计算或对等网络。国内一些媒体将P2P翻译成“点对点”或者“端对端”，学术界则统一称为对等网络（Peer-to-peer networking）或对等计算（Peer-to-peer computing），<a id="more"></a>其可以定义为：网络的参与者共享他们所拥有的一部分硬件资源（处理能力、存储能力、网络连接能力、打印机等），这些共享资源通过网络提供服务和内容，能被其它对等节点（Peer）直接访问而无需经过中间实体。在此网络中的参与者既是资源、服务和内容的提供者（Server），又是资源、服务和内容的获取者（Client）。</p>
<p>　　目前主流的客户端/服务器（Client/Server）结构中，客户端和服务器之间的关系是主从关系。在C/S中，效率=服务器的数量/客户端的数量，随着客户端的数量的急速增长，效率越来越低。因此一种解决方法是将一部分客户端转变成服务器，这样效率就会增大。即若用户N1从服务器S下载10%的文件之后就可以作为服务器来供其他用户使用。因此，将C变成S就是P2P对等网络的本质。</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>P2P</tag>
      </tags>
  </entry>
  <entry>
    <title>Git的使用</title>
    <url>/2020/12/15/git%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h1><ul>
<li>创建空目录：<code>mkdir xxxxx（文件夹名）</code></li>
<li>创建仓库：<code>git init</code></li>
</ul>
<p>　　使用Notepad++创建一个.txt文件并保存到刚刚文件夹下，如readme.txt</p>
<a id="more"></a>

<ul>
<li>将文件添加到仓库：<code>git add readme.txt</code></li>
<li>把文件提交到仓库：<code>git commit -m &quot;xxxxx&quot;</code>（-m后面输入的是本次提交的说明，这样就能从历史记录里方便地找到改动记录。）</li>
</ul>
<p>　　git commit命令执行成功后如下图：</p>
<p><img src="http://jiajunjun.top/md_picture/usegit1.png" alt="git commit命令执行成功"></p>
<p>　　(1 file changed：1个文件被改动（新添加的readme.txt文件）; 2 insertions：插入了两行内容（readme.txt有两行内容）)</p>
<p>　　commit可以一次提交多个文件，所以可以多次add再commit。</p>
<p><img src="http://jiajunjun.top/md_picture/usegit2.png" alt="git commit命令执行成功"></p>
<ul>
<li>查看仓库当前的状态：<code>git status</code></li>
</ul>
<p>　　例如：当对readme.txt进行修改后，输入git status。输出表明修改过但没提交。<br><img src="http://jiajunjun.top/md_picture/usegit3.png" alt="git commit命令执行成功"></p>
<ul>
<li><p>查看修改的内容：<code>git diff</code>。可以看到绿色字体与红色字体的对比，添加了distributed。<br><img src="http://jiajunjun.top/md_picture/usegit4.png" alt="git commit命令执行成功"></p>
</li>
<li><p>进行提交：<code>git add readme.txt</code></p>
</li>
<li><p><code>git commit -m &quot;add distributed&quot;</code></p>
</li>
<li><p>查看此时仓库的状态： <code>git status</code> (显示 nothing to commit,working tree clean，表明当前没有需要提交的修改，而且工作目录是干净的。)</p>
</li>
<li><p>查看提交日志：<code>git log(完整版）</code> (<code>git log --pretty=oneline</code>(精简版）)</p>
</li>
</ul>
<h1 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h1><ul>
<li><p>回退到上一个版本：<code>git reset --hard HEAD^</code> (文件已经被修改回上一个版本，若想回到上上个版本，用HEAD^^，若想回到往上100个版本，用HEAD~100)</p>
</li>
<li><p>又想回到刚刚最新的版本：git reset –hard 6d1fe (从命令行往上翻，找到最新版本前边的commit id，输入前几位就OK了，这里最新版本的commit id 是 6d1fe770…）</p>
</li>
<li><p>如果忘记commit id：git reflog（可以查看之前的每一次命令，可以找到每一次命令的commit id )</p>
</li>
</ul>
<h1 id="工作区和暂存区"><a href="#工作区和暂存区" class="headerlink" title="工作区和暂存区"></a>工作区和暂存区</h1><p><img src="http://jiajunjun.top/md_picture/usegit5.png" alt="git commit命令执行成功"></p>
<p>　　git add 实际上是把文件修改添加到暂存区（图中的stage)，git commit 实际上是把暂存区中的所有内容提交到当前分支（master是Git自动创建的第一个分支）。所以commit可以一次性提交多个文件。如果修改readme.txt文件，并且新建了一个LICENSE文件，此时若执行完git add readme.txt和git add LICENSE后，暂存区的状态如下图：</p>
<p><img src="http://jiajunjun.top/md_picture/usegit6.png" alt="git commit命令执行成功"></p>
<p>　　此时执行git commit后，就变成了下图，暂存区没有任何内容，commit将两个文件全部提交。</p>
<p><img src="http://jiajunjun.top/md_picture/usegit7.png" alt="git commit命令执行成功"></p>
<h1 id="撤销修改"><a href="#撤销修改" class="headerlink" title="撤销修改"></a>撤销修改</h1><ul>
<li><p>文件提交前，可手动修改到之前的状态，也可使用 <code>git checkout -- readme.txt</code> ，将readme.txt文件在工作区的修改撤销，回退到最近一次 git commit 或 git add 的状态。</p>
</li>
<li><p>文件已经add，但未commit：<code>git reset HEAD readme.txt</code>，回退版本，把暂存区的修改退回到工作区。</p>
</li>
</ul>
<h1 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h1><ul>
<li><code>rm test.txt</code><ul>
<li>确实要删除：<code>git rm test.txt</code><br>　<code>git commit -m &quot;xxxxx&quot;</code></li>
<li>删错了：<code>git checkout -- test.txt</code>
　　　　</li>
</ul>
</li>
</ul>
<h1 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h1><ul>
<li><p>在GitHub中新建一个仓库后，在本地仓库下运行：<code>git remote add origin git@github.com:JiaJunJun/learngit.git</code></p>
</li>
<li><p>第一次推送master分支的所有内容：<code>git push -u origin master</code></p>
</li>
<li><p>之后提交：<code>git push origin master</code></p>
</li>
</ul>
<p>　　本地仓库与远程仓库同步</p>
<ul>
<li>把远程仓库克隆到本地：github中新建仓库newlocal，在本地想要位置下输入命令：<code>git clone git@github.com:JiaJunJun/newlocal.git</code></li>
</ul>
<h1 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h1><ul>
<li><p>查看分支：<code>git branch</code></p>
</li>
<li><p>创建分支：<code>git branch &lt;name&gt;</code></p>
</li>
<li><p>切换分支：<code>git checkout &lt;name&gt;</code> 或 <code>git switch &lt;name&gt;</code></p>
</li>
<li><p>创建+切换分支：<code>git checkout -b &lt;name&gt;</code> 或 <code>git switch -c &lt;name&gt;</code></p>
</li>
<li><p>合并某分支到当前分支：<code>git merge &lt;name&gt;</code></p>
</li>
<li><p>删除分支：<code>git branch -d &lt;name&gt;</code></p>
</li>
</ul>
<p>参考链接：<a href="https://www.liaoxuefeng.com/wiki/896043488029600">https://www.liaoxuefeng.com/wiki/896043488029600</a></p>
]]></content>
      <categories>
        <category>开发环境</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>在markdown中插入图片</title>
    <url>/2020/12/02/markdown%E4%B8%AD%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87/</url>
    <content><![CDATA[<h1 id="在markdown文件中插入图片"><a href="#在markdown文件中插入图片" class="headerlink" title="在markdown文件中插入图片"></a>在markdown文件中插入图片</h1><p>　　这里选择采用将图片上传到GitHub上的方法进行图片的插入。具体步骤如下：</p>
<p>　　<a id="more"></a></p>
<p>　　1.登录GitHub,创建一个新仓库用来存放图片，如下图所示：<div align=center><img src="http://jiajunjun.top/md_picture/test_pic1.png" alt="图1"></div></p>
<p>　　2.在本地磁盘中新建一个存放图片的文件夹，然后把需要插入的图片放入该文件夹中。</p>
<p>　　3.在该文件夹下打开Git Bash。</p>
<p>　　4.配置自己的身份：依次输入 </p>
<p>　　<code>git config --global user.name &quot;Your name&quot;</code> </p>
<p>　　<code>git config --global user.email &quot;Your email&quot;</code></p>
<p>　　5.创建代码仓库：</p>
<p>　　<code>git init</code>：此时本地仓库就有一个.git的隐藏文件夹</p>
<p>　　6.提交本地代码：</p>
<p>　　<code>git add .</code> ：把所有文件添加到仓库</p>
<p>　　<code>git commit -m &quot;xxx&quot;</code>：把文件提交到仓库，xxx可以输入本次提交的说明</p>
<p>　　如下图所示：<br>　　<br>　　<div align=center><img src="http://jiajunjun.top/md_picture/test_pic2.png" alt="图2"></div></p>
<p>　　7.把本地仓库关联到GitHub上的远程库：</p>
<p>　　<code>git remote add origin git@github.com:your username/repositories name.git</code>：注意要将your username改成你自己的GitHub账户名，repositories name改成仓库名。</p>
<p>　　8.把本地库的内容推送到远程：</p>
<p>　　<code>git push -u origin master</code></p>
<p>　　此时本地库和远程库的内容就已经一样了。</p>
<p>　　9.打开在GitHub上创建的仓库，进入Settings，下拉找到GitHub Pages，将Source中的None改为master，点击save保存。如下图所示：<div align=center><img src="http://jiajunjun.top/md_picture/test_pic3.png" alt="图3"></div></p>
<p>　　10.此时图片在网络上的地址为GitHub项目地址加图片名，例如我这里的图片地址为：<a href="https://jiajunjun.github.io/md_picture/pic1.png">https://jiajunjun.github.io/md_picture/pic1.png</a></p>
<p>　　11.在markdown文件中插入图片：</p>
<p>　　<code>![图片名称](https://jiajunjun.top/md_picture/pic1.png)</code></p>
<p>　　如果还不能显示，将https://改为http://即可</p>
<p>　　<div align=center><img src="http://jiajunjun.top/md_picture/pic1.png" alt="pic1"></div></p>
<p>　　12.之后如果想要再插入新的图片时，可以在本地库中添加图片，然后在本地库的文件夹下打开Git Bash，然后依次输入：</p>
<p>　　<code>git add .</code></p>
<p>　　<code>git commit -m &quot;xxxx&quot;</code></p>
<p>　　<code>git push origin master</code></p>
<p>　　之后按上述方法在markdown文件中插入图片即可。</p>
]]></content>
      <categories>
        <category>TIPS</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么要开发TCP/IP协议</title>
    <url>/2020/12/04/TCPIP/</url>
    <content><![CDATA[<p>TCP/IP协议的作用是使计算机实现互联共享。</p>
<a id="more"></a>

<p>　　世界上有很多台计算机，多台计算机之间需要进行通信，因此Internet可以将不同地区的计算机相互连接。但是每台计算机都使用不同的操作系统，为了使这些互连的计算机之间能够进行通信，他们必须使用同样的“语言”。就像世界各地的人都使用不同的语言，当他们需要进行交流的时候就需要一个共同的语言以便沟通，这个共同的“语言”就是协议。只要使用相同的协议便可以实现通信。</p>
<p>　　因此，TCP/IP(Transmission Control Protocol/Internet Protocol)是指能够在多个不同的网络间实现传输的协议簇。也就是说，TCP/IP 是互联网相关各类协议族的总称。</p>
<p>　　TCP/IP五层模型为：<img src="http://jiajunjun.top/md_picture/Tcpip.png" alt="图2"></p>
<p>　　物理层：该层负责比特流在节点之间的传输，即负责物理传输。</p>
<p>　　数据链路层：控制网络层与物理层之间的通信，主要功能是保证物理线路上进行可靠的数据传递。</p>
<p>　　网络层：决定如何将数据从发送方路由到接收方。网络层通过综合考虑发送优先权，网络拥塞程度，服务质量以及可选路由的花费等来决定从网络中的A节点到B节点的最佳途径。即建立主机到主机的通信。</p>
<p>　　传输层：该层为两台主机上的应用程序提供端到端的通信。</p>
<p>　　应用层：应用程序收到传输层的数据后，接下来就要进行解读。解读必须事先规定好格式，而应用层就是规定应用程序的数据格式。主要的协议有：HTTP.FTP,Telent等。</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title>要提供可靠的传输要解决哪些问题，如何解决？</title>
    <url>/2020/12/17/%E5%8F%AF%E9%9D%A0%E4%BC%A0%E8%BE%93/</url>
    <content><![CDATA[<p>思考：</p>
<ul>
<li><p>TCP层实现可靠传输中面向连接和无连接各自的优点和缺点</p>
</li>
<li><p>什么是端到端的可靠传输</p>
<a id="more"></a></li>
<li><p>数据链路层和TCP层解决可靠传输的区别</p>
<ul>
<li>TCP层解决端到端的可靠传输，而DLL层解决一跳内的可靠传输。</li>
</ul>
</li>
</ul>
<p>　　计算机网络的可靠传输从核心网外移，在端系统解决。不可靠的传输可能会出现：1.传输错误的信息；2.重复信息；3.信息丢失；4.超时。针对上述问题，有以下解决方法：</p>
<ul>
<li>1.解决信息错误：</li>
</ul>
<p>　　解决信息错误包括两个部分：检错和纠错。</p>
<p>　　检错：发送方除了发送所需的信息外，还需要额外发送一部分用来检错，例如奇偶校验法。在最后添加一位检错位（0或1），最终使得整条数据的1的个数为偶（或奇）。</p>
<p>　　纠错：</p>
<p>　　1.重传：依赖于发送方的配合，简单易于实现，但耗费时间长。</p>
<p>　　2.前向纠错（FEC)：无时延，依赖复杂的编码技术，开销大。（前向纠错是一种差错控制方式，它是指信号在被送入传输信道之前预先按一定的算法进行编码处理，加入带有信号本身特征的冗码，在接收端按照相应算法对接收到的信号进行解码，从而找出在传输过程中产生的错误码并将其纠正的技术）</p>
<p>　　3.第一种和第二种方法结合（混合技术）</p>
<ul>
<li>2.解决重复信息：</li>
</ul>
<p>　　为每组信息加上编号，信息重复则编号重复，编号重复的信息直接丢弃。</p>
<ul>
<li>3.解决信息丢失：</li>
</ul>
<p>　　有了编号后就可以通过编号的顺序判断数据的丢失情况。</p>
<ul>
<li>4.解决超时的问题：</li>
</ul>
<p>　　信息传输过程中需要等待对方的确认信息，互相等待会导致死锁。解决方法是使用定时器，若超过预定时间，将进行重传。此为停止等待协议，但该协议存在的问题是大部分的时间都用来等待，信道的利用率低，因此可以采用流水线机制，发送方不停地发数据，接收方的接受能力要与发送方的发送能力匹配，需要进行流量控制和拥塞控制。</p>
<p>　　流量控制：滑动窗口，连续ARQ协议。</p>
<p>　　拥塞控制：产生拥塞的原因是对资源的需求总和&gt;可用资源。拥塞控制方法有：慢开始和拥塞避免、快重传和快恢复。</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>可靠传输</tag>
      </tags>
  </entry>
  <entry>
    <title>案例学习1：基于图像的水质识别</title>
    <url>/2021/01/09/%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A01%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E7%9A%84%E6%B0%B4%E8%B4%A8%E8%AF%86%E5%88%AB/</url>
    <content><![CDATA[<h1 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h1><p>已有数据集根据专家经验将水质分为5个类别，要求根据水质图片能够实现水质的分类。</p>
<p>案例与教程来源：人邮教师学院大数据（python方向）教学能力提升训练营（第2期）</p>
<a id="more"></a>

<h1 id="求解思路"><a href="#求解思路" class="headerlink" title="求解思路"></a>求解思路</h1><ul>
<li>读取图片数据</li>
<li>截取图片中心区域</li>
<li>提取颜色特征</li>
<li>模型构建与评估</li>
</ul>
<h1 id="过程要点"><a href="#过程要点" class="headerlink" title="过程要点"></a>过程要点</h1><p><strong>图像的读取</strong></p>
<p><code>from PIL import Image</code><br><code>path = &#39;...&#39;</code><br><code>img = Image.open(path)</code></p>
<p>上述代码可以读取一张图片，若要读取文件夹内所有图片，则可以使用循环。path赋值为图片的公共路径，即所在的文件夹的路径，之后每幅图片用“path+img_names[i]”表示，其中img_names中保存所有图片的名称。通过使用正则化提取所有图片的名称，并添加到img_names列表中。</p>
<p><strong>截取图片中心区域</strong></p>
<p>使用<code>img.crop((左上点横坐标，左上点纵坐标，右下角横坐标，右下角纵坐标))</code>截取某一区域</p>
<p><strong>读取图像的颜色特征</strong></p>
<p>颜色特征采用颜色矩的方法，颜色矩包括各个颜色通道的一阶矩，二阶矩和三阶矩，因此一个RGB的图像共有9个特征分量。</p>
<p><img src="http://jiajunjun.top/md_picture/renyoucase/color_moment.png" alt="图1"></p>
<p><code>r, g, b = img.split()</code>(分成三个颜色通道）</p>
<p><code>rd = np.asarray(r)</code><br><code>gd = np.asarray(g)</code><br><code>bd = np.asarray(b)</code>(取出各通道的像素值）</p>
<p><code>rd.mean()</code>（一阶矩）</p>
<p><code>rd.std()</code>(二阶矩)</p>
<p><code>mid = np.mean((rd-rd.mean())**3)</code>  <code>np.sign(mid)*abs(mid)**(1/3)</code>（三阶矩的计算方法）</p>
<p>其中需要使用np.sign(mid)用来表示mid的符号，取值为-1,0,1。否则开三次根的时候，若为负数会出错。</p>
<p>完整代码：</p>
<p><a href="https://github.com/JiaJunJun/case_study/tree/master/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E7%9A%84%E6%B0%B4%E8%B4%A8%E8%AF%86%E5%88%AB">https://github.com/JiaJunJun/case_study/tree/master/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E7%9A%84%E6%B0%B4%E8%B4%A8%E8%AF%86%E5%88%AB</a></p>
]]></content>
      <categories>
        <category>案例学习</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>图像处理</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>案例实践1：手写数字识别</title>
    <url>/2021/01/10/%E6%A1%88%E4%BE%8B%E5%AE%9E%E8%B7%B51%EF%BC%9A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</url>
    <content><![CDATA[<h1 id="实践过程遇到的问题"><a href="#实践过程遇到的问题" class="headerlink" title="实践过程遇到的问题"></a>实践过程遇到的问题</h1><p>1.教程中图片读取使用的是cv2。cv2和pillow.Image的区别如下：</p>
<a id="more"></a>

<ul>
<li>打开：<code>cv2.imread()</code>（BGR） <code>Image.open()</code>（RGB)</li>
<li>查看图片尺寸：<code>cv2.shape</code> <code>Image.size</code></li>
<li>保存：<code>cv2.inwrite(path,img)</code> <code>Image.save(path)</code></li>
</ul>
<p>2.读取图片时需要注意一下图片的尺寸，由于输入模型要使用同样的尺寸，因此在本案例中需要对图片进行压缩以实现同样尺寸，使用cv2.resize。</p>
<p>3.本例中对于RGB三个通道的区分意义不大，因为都是白纸黑字，只需要取其中一个通道的特征就好了。</p>
<p>4.采用机器学习的决策树算法效果不好，可使用MLPClassfier，之后的学习中可采用CNN等其他神经网络。</p>
<h1 id="个人代码"><a href="#个人代码" class="headerlink" title="个人代码"></a>个人代码</h1><p>my_data_process.py是看教程之前自己编写的数据处理程序，多是仿照上节课的案例进行实践，没有进行深入的思考，对于数据处理应当考虑不同案例的特殊性。例如RGB三通道的问题。</p>
<p>my_main.py中只读取了测试数据和训练数据，由于找不到好的训练方法，故没有进一步编写。</p>
<p>完整代码：<br><a href="https://github.com/JiaJunJun/case_study/tree/master/%E6%A1%88%E4%BE%8B%E5%AE%9E%E8%B7%B51%EF%BC%9A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB">https://github.com/JiaJunJun/case_study/tree/master/%E6%A1%88%E4%BE%8B%E5%AE%9E%E8%B7%B51%EF%BC%9A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB</a></p>
]]></content>
      <categories>
        <category>案例实践</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>图像处理</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读：Understanding Dropouts in MOOCs</title>
    <url>/2020/12/01/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><strong>objective：</strong></p>
<p>　　MOOCs有极高的辍学率，完成率低于5% 。本文研究导致用户退出的主要因素，用户在MOOCs学习的主要动机。</p>
<a id="more"></a>

<p><strong>methods：</strong></p>
<p>　　将用户的学习行为分成几个不同的类别。揭示不同课程的辍学者之间的高度相关性以及朋友辍学行为之间的强烈影响。在此基础上，提出了一个上下文感知的特征交互网络(CFIN)来建模和预测用户的退出行为。CFIN利用上下文平滑技术平滑不同上下文的特征值，并利用注意机制将用户和课程信息结合到建模框架中。</p>
<p><strong>results：</strong></p>
<p>　　CFIN比现有的几种方法具有更好的性能。所提出的方法模型已经在实际系统上部署，以帮助提高用户保留率。</p>
<hr>
<h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><p>　　KDDCUP、XuetangX</p>
<hr>
<h1 id="Insights"><a href="#Insights" class="headerlink" title="Insights"></a>Insights</h1><h2 id="Temporal-Code"><a href="#Temporal-Code" class="headerlink" title="Temporal Code"></a>Temporal Code</h2><p>　　Suc:学生u和登记的课程c，时间代码是一个二值向量，每个值表示用户u在第k周是否访问课程c。</p>
<p>　　Su:学生u的每一项课程在每周的访问情况</p>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><p>　　将所有用户时间编码的稀疏表示输入到K-means算法中。</p>
<p>　　通过Silhouette Analysis(1987)，集群的数量被设置为5。</p>
<h2 id="相关性"><a href="#相关性" class="headerlink" title="相关性"></a>相关性</h2><ul>
<li><strong>课程之间的相关性（回归分析）：</strong></li>
</ul>
<p>　　方法：用户在课程中的退学行为被编码为16-dim虚拟向量，每个元素表示用户在相应的一周内是否访问过该课程(因此16对应学习该课程的16周)。回归模型的输入和输出是两个虚拟向量，表示用户在同一学期中对两门不同课程的退学行为。</p>
<p>　　结果：同一类别的课程之间的相关性要高于不同类别的课程。一种可能的解释是，当用户学习MOOC的时间有限时，他们可能会首先放弃自己的课程，而选择补充知识领域的课程。</p>
<ul>
<li><strong>辍学朋友的影响（基于网络）：</strong></li>
</ul>
<p>　　方法：建立用户-课程图，节点是用户和课程，边是代表参加了课程。用随机游走算法为图中的每个用户和每个课程进行低维表示。计算了选修同一门课程的用户间的余弦相似度。相似度大于0.8视为好友。</p>
<p>　　结果:结果表明用户的退学率很大程度上受到其朋友的退学行为的影响</p>
<hr>
<h1 id="CFIN算法"><a href="#CFIN算法" class="headerlink" title="CFIN算法"></a>CFIN算法</h1><h2 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h2><ul>
<li><p><strong>Enrollment Relation（登记关系）：</strong> U,C,E</p>
</li>
<li><p><strong>Learning Activity（学习活动）：</strong>X（u,x）</p>
</li>
<li><p><strong>Context Information（上下文信息）：</strong>Z（u,c）包括用户信息（性别，年龄，位置，学历信息，用户聚类）和课程信息（课程类别）</p>
</li>
</ul>
<h2 id="目标问题"><a href="#目标问题" class="headerlink" title="目标问题"></a>目标问题</h2><p>　　给定用户u在之前的关于课程c的学习活动<strong>X(u，c)**和上下文信息</strong>Z(u,c)**，判断用户在perdiction period是否有学习活动，用y(u,c)表示。</p>
<h2 id="Context-aware-Feature-Interaction-Network（感知上下文的特征交互网络）"><a href="#Context-aware-Feature-Interaction-Network（感知上下文的特征交互网络）" class="headerlink" title="Context-aware Feature Interaction Network（感知上下文的特征交互网络）"></a>Context-aware Feature Interaction Network（感知上下文的特征交互网络）</h2><ul>
<li><strong><em>Motivation</em>:</strong></li>
</ul>
<p>　　上下文平滑：使用卷积神经网络(CNN)通过利用上下文统计来学习每个活动特征**xi(u, c)**的上下文感知表示</p>
<p>　　注意机制：通过将*<em>Z(u, c)**</em>纳入到退学预测中，来学习不同活动的重要性。</p>
<ul>
<li><strong><em>上下文平滑策略</em>:</strong></li>
</ul>
<p>　　三个步骤：特征增强、嵌入和特征融合。</p>
<pre><code>+ **特征增强：** **xi（u，c)**加上用户u对所有课程的统计信息，和课程在所有用户上的统计信息。

+ **嵌入：****xi**通过嵌入层转化为稠密向量,乘以参数a表示嵌入向量。

+ **特征融合：**将嵌入矩阵Egi通过CNN变成一个向量,Vgi=σ（Wδ(Egi)+b)

**每个特征组Xgi都被一个稠密向量Vgi表示，它可以被看作是每个xi的上下文感知的表示，并集成了其上下文统计信息。**</code></pre>
<ul>
<li><strong><em>注意机制</em>:</strong></li>
</ul>
<p>　　将Z的嵌入矩阵Ez输入到全连通层中，将Z转换为密集向量Vz：Vz=σ（Wδ(Ez)+b)</p>
<p>　　使用Vz计算每个Vgi的注意力分数λi</p>
<p>　　得到λi*Vgi的加权和Vg sum,作为X(u,c)的上下文感知表示</p>
<p>　　将Vg sum放入l层的DNN中学习特征间的相互作用，最后用sigmoid函数预测</p>
<h2 id="Model-Ensemble（模型集成）"><a href="#Model-Ensemble（模型集成）" class="headerlink" title="Model Ensemble（模型集成）"></a>Model Ensemble（模型集成）</font></h2><p><strong>xgboost结合CFIN</strong></p>
<hr>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>　　使用TensorFlow实现CFIN，采用Adam对模型进行优化。为了避免过拟合，对权值矩阵应用l2正则化。采用直线直线单元(Relu)作为激活函数。所有特性在输入到CFIN之前都经过了标准化。</p>
<p>　　在KDDCUP和XuetangX数据集上测试了CFIN的性能。对于KDDCUP数据集，比赛组织者将历史周期和预测周期分别设置为30天和10天。没有对该数据使用CFIN的注意机制，因为数据集中没有提供上下文信息。对于XuetangX数据集，历史周期设置为35天，预测周期设置为10天，即Dh=35, Dp=10</p>
<h2 id="对比算法"><a href="#对比算法" class="headerlink" title="对比算法"></a>对比算法</h2><ul>
<li> LR</li>
<li> SVM</li>
<li> RF</li>
<li> GBDT</li>
<li> DNN</li>
<li> CFIN</li>
<li> CFIN-en</li>
</ul>
<p>　　基于网格搜索的5次交叉验证(CV)调整参数，并在所有实验中使用最优的参数组。评价指标包括 AUC和F1。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><ul>
<li>预测结果</li>
</ul>
<p>　　CFIN-en在两个数据集上的表现都是最好的，在KDDCUP数据集上AUC得分达到90.93%，CFIN也表现出更好的性能。</p>
<ul>
<li>特征贡献</li>
</ul>
<p>　　对三个主要活动特征进行了特征消融实验，即视频活动、任务活动和论坛活动。具体来说，我们首先将所有特性输入到CFIN，然后逐个删除每种类型的活动特性，以观察性能的变化。</p>
<p>　　结果表示，这三种活动在这个任务中都是有用的。在KDDCUP上，作业是最重要的，而在XuetangX上，视频是最重要的。</p>
<ul>
<li>不同用户组的不同特性的细粒度分析</li>
</ul>
<p>　　五组群体的注意权重分布有很大的不同</p>
]]></content>
      <categories>
        <category>教育数据挖掘</category>
      </categories>
      <tags>
        <tag>教育数据挖掘</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>从面向连接和无连接的角度分析，计算机网络为什么没有采用通信网络的方法和模式？</title>
    <url>/2020/12/04/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%97%A0%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<ul>
<li>面向连接：在通信时事先建立好一条虚拟通信线路</li>
<li>无连接：不需事先建立线路，每个分组自主选择线路传输到目标地址<a id="more"></a>
　　通信网络采用面向连接的方式，使用电路交换，由核心网链接各种终端和设备，而计算机网络的可靠传输从核心网外移，在核心网中只提供互连的作用，因此可靠传输如何实现是计算机网络需要考虑的问题。计算机网络没有采用面向连接的原因有：</li>
</ul>
<p>　　1.二者终端设备的性能差别大</p>
<p>　　通信网络的终端不具备差错处理和计算能力，需要采用面向连接来保证通信的质量。而计算机网络的终端是只能的主机，如果传输过程中出现了差错，终端也有办法实现可靠传输。</p>
<p>　　2.即使网络传输正确，端到端的数据处理也有可能出现错误。在传输层中使用TCP面向连接的协议用来保障可靠传输。网络层使用无连接的方法。</p>
<p>　　3.计算机网络的需求更多，如果采用面向连接的方法所需成本较高。</p>
<p>　　这也是201电话比之前电话便宜一半的原因。</p>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>面向连接和无连接</tag>
      </tags>
  </entry>
</search>
